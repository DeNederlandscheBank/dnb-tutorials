{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nafigator Tutorial\n",
    "This tutorial helps you to get started with the Nafigator package. We will:\n",
    "\n",
    "1. Create a naf file\n",
    "2. Retrieving information from NAF files\n",
    "3. Store your NAF files\n",
    "\n",
    "This tutorial is set up for one pdf file. You can also import multiple files at the same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import your packages\n",
    "import nafigator\n",
    "import os\n",
    "import stanza\n",
    "import pandas as pd\n",
    "from nafigator.parse2naf import generate_naf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and specify your (English) NLP engine\n",
    "stanza.download('en')\n",
    "stanza_nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create a naf file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the length of your document, the creation of a NAF file containing all the relevant layers may take a while.\n",
    "In this example we use \"DNB's annual report 2020\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the NAF file with an input (pdf) file. The data folder contains an example for you to use, but you can specify the file you want to analyse by changing 'input'.\n",
    "file_name = \"../data/external/naf/annual_report_dnb_2020.pdf\"\n",
    "if not os.path.exists(file_name):\n",
    "    doc = generate_naf(input = file_name,\n",
    "                       engine = \"stanza\",\n",
    "                       language = \"en\",\n",
    "                       naf_version = \"v3.1\",\n",
    "                       dtd_validation = False,\n",
    "                       params = {'fileDesc': {'author': 'anonymous'}},\n",
    "                       nlp = stanza_nlp)\n",
    "else:\n",
    "    doc = nafigator.NafDocument().open(file_name[:-3]+\"naf.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have now your first NAF file. To access the plain text of the document, run the below command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Retrieving information from naf documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with NAF, it's important to understand the structure of the naf.xml file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The raw layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw layer contains the complete string of the document without annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.raw[0:122]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The header layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The header layer contains all meta data of the naf file: file description, public information and information about the linguistic processors used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get information about the the layers in the NAF file, use the header function:\n",
    "doc.header.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.header['fileDesc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Public data definitions follow the Dublin Core Metadata Initiative: http://purl.org/dc/elements/1.1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.header['public']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents are parsed with a NLP engine (in this case stanza) consisting of different pipeline elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.header['linguisticProcessors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each layer is a different dictionary containing a list.\n",
    "For more examples, please check: https://pypi.org/project/nafigator/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use doc.text to access the following elements from the text layer:\n",
    "- id: the id of the word form\n",
    "- sent: sentence id of the word form\n",
    "- para: paragraph id of the word form\n",
    "- offset: the offset (in charachters) of the word form\n",
    "- length: the length (in charachters) of the word form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To access a specific part of the document, \n",
    "# you can use lists to go through the entire file. \n",
    "# Let's say you want to access the 4240th word in this file.\n",
    "doc.text[4239]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to extract specific values from the file (for example, the page number where the word occurs)\n",
    "doc.text[4239][\"page\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The offset is aligned with the raw layer, so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.raw[24016:24016+7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The terms layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terms layer contains linguistic and morphological properties of each word.\n",
    "- id: the id of the term\n",
    "- type: open or closed term\n",
    "- lemma: the lemma of the term\n",
    "- pos: the part-of-speech of the term\n",
    "- morphofeat: the morphological features of the term\n",
    "- span: the ids of the wordform of this term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.terms[4239]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the number in the list is identical to the number in the list of the wordform, but in general this is not the case. Corresponding wordforms should be retrieved with the span in the term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The entities layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the different type of entities that are found\n",
    "print(set([entity['type'] for entity in doc.entities]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the first entities that are recognized by the NLP engine\n",
    "print([entity['text'] for entity in doc.entities[0:100] if entity['type']=='ORG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard NLP engine make errors in recognizing entities:\n",
    "print([entity['text'] for entity in doc.entities[0:100] if entity['type']=='WORK_OF_ART'])\n",
    "# (although some might disagree in this case :) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dependency layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key word search\n",
    "You can search NAF documents in several ways. Here we'll show you two types:\n",
    "- Exact search\n",
    "- Lemmatized search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The 4239th word is 'economy'. If you want to search all sentences that contain this word, you loop through the entire text file as shown below. This will show you all the sentences that contain 'economy'.\n",
    "print([word[\"id\"] for word in doc.text if word[\"text\"]==\"economy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatized Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the first word id of all term where lemma is economy\n",
    "print([term['span'][0]['id'] for term in doc.terms if term['lemma']=='economy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve one of the word ids that did not come up with the exact search\n",
    "print([word['text'] for word in doc.text if word[\"id\"]==\"w985\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting more from page text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the text on the 22nd page\n",
    "print(\" \".join([word['text'] for word in doc.text if word['page'] == '22']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Specific Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing a sentence based on a sentence number\n",
    "sentence =  doc.sentences[23]\n",
    "print(\"Sentence: \" + str(sentence[\"text\"])+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Storing your NAF File\n",
    "After you have generated your NAF file, you probably want to store it for later use. Especially since this is the most time consuming part of your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store you document naf file as xml \n",
    "doc.write(\"../data/external/naf/annual_report_dnb_2020.naf.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to reuse it later, import an existing naf file as shown below\n",
    "doc_name = os.path.join(\"..\", \"data\", \"external\", \"naf\", \"annual_report_dnb_2020.naf.xml\")\n",
    "#doc = nafigator.NafDocument().open('notebook_data/output.naf.xml')\n",
    "doc = nafigator.NafDocument().open(doc_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
