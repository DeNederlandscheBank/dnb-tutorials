{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse public Solvency 2 data of Dutch insurance undertakings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! In this tutorial we will use the public Solvency 2 data of all Dutch insurance undertakings and present it in one large DataFrame. In doing so, we are able to use some easy but powerful machine learning algorithms to analyze the data.\n",
    "\n",
    "Solvency 2 data of individual insurance undertakings are published yearly by the Statistics department of DNB. The data represents the financial and solvency situation of each insurance undertaking at the end of each year. Because Solvency 2 came into effect in 2016, we currently have three years of data: ultimo 2016, ultimo 2017 and ultimo 2018. \n",
    "\n",
    "The publication of the data is in the form of an Excel file with a number of worksheets containing the aggregated data and the individual data. In this tutorial we will use the individual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pyplot\n",
    "from datetime import datetime\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the data published on the DNB website in an Excel file\n",
    "\n",
    "https://statistiek.dnb.nl/en/downloads/index.aspx#/?statistics_type=toezichtdata&theme=verzekeraars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(\"../data/Data individual insurers (year).xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Excel file contains several worksheets with data. We want to combine all the data together in one DataFrame. To do that we need some data preparation and data cleaning for each worksheet. This is done by the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sheet(num):\n",
    "    \"\"\"This function extracts the data from a worksheet and puts it in a DataFrame and the column names are set \n",
    "    to lower case. An index of the DataFrame is set to the insurance undertaking name and the reporting period. Then \n",
    "    we perform some cleaning (the original worksheets contain some process information). In addition, some worksheets \n",
    "    in the file contain 2-dimensional data, that need to be pivoted such that we obtain one large vector with all the \n",
    "    data per insurance undertaking in one row.\n",
    "    \n",
    "    Parameters:\n",
    "    num (int): number of the worksheet\n",
    "    \n",
    "    Returns: \n",
    "    dataframe: cleaned and indexed dataframe with all data per insurance undertaking in one row\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # read entire Excel sheet\n",
    "    df = xls.parse(num)\n",
    "\n",
    "    # columns names to lower case\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "    # set index to name and period\n",
    "    df.set_index(['relatienaam', 'periode'], inplace = True)\n",
    "\n",
    "    # data cleaning (the excel sheet contains some additional data that we don't need)\n",
    "    drop_list = [i for i in df.columns if 'unnamed' in i or 'selectielijst' in i]\n",
    "    df.drop(drop_list, axis = 1, inplace = True)\n",
    "    \n",
    "    # pivot DataFrame\n",
    "    if \"row_name\" in df.columns:\n",
    "        df.drop(\"row_name\", axis = 1, inplace = True)\n",
    "        df = df.pivot(columns = 'row_header')\n",
    "\n",
    "    if df.columns.nlevels > 1:\n",
    "        df.columns = [str(df.columns[i]) for i in range(len(df.columns))]\n",
    "\n",
    "    # remove duplicate rows\n",
    "    df = df[~(df.index.duplicated())]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating one large vector per insurance undertaking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function above we can read a part of the Excel file and store it in a Pandas DataFrame. The following worksheets are contained in the Excel file published by DNB.\n",
    "\n",
    "* Worksheet 14: balance sheet\n",
    "* Worksheet 15: premiums - life\n",
    "* Worksheet 16: premiums - nonlife\n",
    "* Worksheet 17: technical provisions - 1\n",
    "* Worksheet 18: technical provisions - 2\n",
    "* Worksheet 19: transition and adjustments\n",
    "* Worksheet 20: own funds\n",
    "* Worksheet 21: solvency capital requirements - 1\n",
    "* Worksheet 22: solvency capital requirements - 2\n",
    "* Worksheet 23: minimum capital requirements\n",
    "* Worksheet 24: additional information life\n",
    "* Worksheet 25: additional information non-life\n",
    "* Worksheet 26: additional information reinsurance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s read the first worksheet with data and then append the other sheets to it. We shall not read the last three worksheets, because these contain the country specific reports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_sheet(14)\n",
    "for l in range(15, 24):\n",
    "    df_temp = get_sheet(l)\n",
    "    df = df.join(df_temp, rsuffix = \"_\"+ str(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the data from the year 2018, and select all columns that have floating numbers in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_18 = df.xs(datetime(2018,12,31), axis = 0, level = 1, drop_level = True)\n",
    "df_18 = df_18[[df_18.columns[c] for c in range(len(df_18.columns)) if df_18.dtypes[c] == 'float64']]\n",
    "df_18.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis solvency ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only consider insurance undertakings with the same book year and insurance undertakings that have reported every year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Bos Fruit Aardappelen Onderlinge verzekeringen BFAO U.A.'], level='relatienaam')\n",
    "years = df.index.get_level_values(1).unique()\n",
    "names = df.index.get_level_values(0).unique()\n",
    "for i in names:\n",
    "    if (((i,years[0]) in df.index) & ((i,years[1]) in df.index) & ((i,years[2]) in df.index)) == False: \n",
    "        df = df.drop(i,level='relatienaam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the solvency ratio for each insurance undertaking in each year. Under Solvency 2, the solvency ratio is\n",
    "the ratio of eligible own funds to required own funds. Required own funds, also referred to as the solvency capital\n",
    "requirement (SCR), constitute a risk-based buffer, based on the actual risks on the balance sheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCR = pd.DataFrame(index = df.index)\n",
    "SCR['assets'] = df['assets|total assets , solvency ii value']\n",
    "SCR['ratio'] = (df['excess of assets over liabilities , solvency ii value'])/df['scr']\n",
    "SCR['weight']=0\n",
    "years = df.index.get_level_values(1).unique()\n",
    "names = df.index.get_level_values(0).unique()\n",
    "for i in names:\n",
    "    for j in years:\n",
    "        SCR.loc[(i,j),'weight'] = SCR.loc[(i,j),'assets']/sum(SCR.xs(j, level = 'periode')['assets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the solvency ratio for each insurance undertaking we also calculate the weighted average solvency ratio of the whole insurance sector for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_t = pd.DataFrame(index=years)\n",
    "ratio_t['ratio'] = 0\n",
    "for j in years:\n",
    "    ratio_t.loc[j,'ratio'] = sum(SCR.xs(j,level = 'periode')['ratio']*SCR.xs(j,level = 'periode')['weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in the development of the solvency ratio over time. We plot the solvency ratio of an individual insurance undertaking - in this case AEGON Levensverzekering - and the weighted average solvency ratio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(years.year, SCR.xs('AEGON Levensverzekering N.V.',level = 'relatienaam')['ratio'], color = 'blue',label = 'Aegon')\n",
    "pyplot.plot(years.year,ratio_t['ratio'], color = 'red', label = 'average sector')\n",
    "pyplot.xticks(np.arange(min(years.year), max(years.year)+1, 1))\n",
    "pyplot.legend()\n",
    "pyplot.title('Solvency ratio over time')\n",
    "pyplot.xlabel('year')\n",
    "pyplot.ylabel('solvency ratio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the most similar insurance undertaking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s apply some algorithms to this data set. Suppose we want to know what insurance undertakings are similar with respect to their financial and solvency structure. To do that we can calculate the distances between all the data points of each insurance undertakings. An insurance undertaking with a low distance to another insurance undertaking might be similar to that undertaking.\n",
    "\n",
    "If we divide each row by the total assets we do as if all insurance undertakings have equal size, and then the distances indicate similarity in financial and solvency structure (and not similarity in size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_18.div(df_18['assets|total assets , solvency ii value'], axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn package provides numerous algorithms to do calculations with distances. Below we apply the NearestNeighbors algorithm to find the neighbors of each insurance undertaking. Then we get the distances and the indices of the data set and store them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(n_neighbors = 2, algorithm = 'brute').fit(X.values)\n",
    "\n",
    "distances, indices = nbrs.kneighbors(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the nearest neighbors of the first ten insurance undertakings in the list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in indices[0:10]:\n",
    "    print(X.index[i[0]] + \" --> \" + X.index[i[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with the shortest distance between two insurance undertakings we can find the two insurance undertakings that have the highest similarity in their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_list = np.where(distances[:,1] == distances[:,1].min())\n",
    "\n",
    "list(X.index[min_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to understand the financial performance it is of course handy to know which insurance undertakings are similar. A more general approach when comparing insurance undertakings is to cluster them into a small number of peer groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of insurance undertakings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we cluster the insurance undertakings based on the 1272-dimensional data? To do this we apply the t-sne algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = TSNE(n_components = 2, perplexity = 5, verbose = 0, random_state = 0).fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize = (7, 7))\n",
    "\n",
    "pyplot.scatter(x = Y[:, 0], \n",
    "              y = Y[:, 1], \n",
    "              s = 7)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 4, random_state = 0, n_init  = 10).fit(Y)\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    if kmeans.labels_[i] == 0:\n",
    "        pyplot.scatter(x = Y[i,0],y = Y[i,1],s = 5,c = 'red')\n",
    "    elif kmeans.labels_[i] == 1:\n",
    "        pyplot.scatter(x = Y[i,0],y = Y[i,1],s = 5,c = 'orange')\n",
    "    elif kmeans.labels_[i] == 2:\n",
    "        pyplot.scatter(x = Y[i,0],y = Y[i,1],s = 5,c = 'blue')\n",
    "    elif kmeans.labels_[i] == 3:\n",
    "        pyplot.scatter(x = Y[i,0],y = Y[i,1],s = 5,c = 'green')\n",
    "#Print the names of some individual insurance undertakings        \n",
    "pyplot.text(Y[1,0]+1, Y[1,1]+1,X.index.get_level_values(0)[1], fontsize=9)\n",
    "pyplot.text(Y[2,0]+1, Y[2,1]+1,X.index.get_level_values(0)[2], fontsize=9)\n",
    "pyplot.text(Y[6,0]+1, Y[6,1]+1,X.index.get_level_values(0)[6], fontsize=9)\n",
    "pyplot.text(Y[7,0]+1, Y[7,1]+1,X.index.get_level_values(0)[7], fontsize=9)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how you zoom in you see different clusters in this picture. The red cluster represents the health insurance undertakings (with more clusters within that set: those offering basic health insurance and other offering additional health insurances, or both). The orange cluster consists of (mostly) non-life insurance undertakings, and the green cluster consists of life insurance undertakings. And both clusters can be divided into several more sub clusters. These clusters can be used in further analysis. For example, you could use these as peer groups of similar insurance undertakings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have a 1272-dimensional vector of each insurance undertaking we might wish somehow to cluster the features in the data set. That is, we want to know which columns belong to each other and what columns are different.\n",
    "\n",
    "An initial form of clustering were the different worksheets in the original Excel file. The data was clustered around the balance sheet, premiums, technical provisions, etc. But can we also find clusters within the total vector without any prior knowledge of the different worksheets?\n",
    "\n",
    "A simple and effective way is to transpose the data matrix and feed it into the t-sne algorithm. That is, instead of assuming that each feature provides additional information about an insurance undertaking, we assume that each insurance undertaking provides additional information about a feature.\n",
    "\n",
    "Let’s do this for only the balance sheet. In a balance sheet it is not immediately straightforward how the left side is related to the right side of the balance sheet, i.e. which assets are related to which liabilities. If you cluster all the data of the balance sheet then related items are clustered (irrespective of whether they are assets or liabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_sheet(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the scaled values we now take whether or not a data point was reported or not, and then transpose the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "X = (df != 0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply the t-sne algorithm. In this case with a lower perplexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = TSNE(n_components = 2, \n",
    "         perplexity = 1.0, \n",
    "         verbose = 0, \n",
    "         random_state = 0, \n",
    "         learning_rate = 20, \n",
    "         n_iter = 10000).fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the result with 15 identified clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize = (7, 7))\n",
    "\n",
    "pyplot.scatter(x = Y[:, 0], \n",
    "              y = Y[:, 1], \n",
    "              s = 5)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 15, random_state = 0, n_init  = 10).fit(Y)\n",
    "\n",
    "for i in range(len(kmeans.cluster_centers_)):\n",
    "    \n",
    "    pyplot.scatter(x = kmeans.cluster_centers_[i,0],\n",
    "                   y = kmeans.cluster_centers_[i,1],\n",
    "                   s = 1,\n",
    "                   c = 'yellow')\n",
    "    \n",
    "    pyplot.annotate(str(i), \n",
    "                   xy = (kmeans.cluster_centers_[i, 0], kmeans.cluster_centers_[i, 1]), \n",
    "                   size = 13)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see are large number of different clusters.\n",
    "\n",
    "In a balance sheet it is not immediately straightforward how the left side is related to the right side of the balance sheet, i.e. which assets are related to which liabilities. If you cluster all the data of the balance sheet then related items are clustered (irrespective of whether they are assets or liabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.T.loc[kmeans.labels_ == 6].index:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the assets held for index-linked and unit-linked contracts are in the same cluster as the technical provisions for index-linked and unit-linked items (and some other related items are found).\n",
    "\n",
    "Besides clustering based on whether a data point was reported or not we now cluster the data related in their changes over time. We only consider insurance undertakings which report with the same book year and have reported every year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_sheet(14)\n",
    "df = df.drop(['Bos Fruit Aardappelen Onderlinge verzekeringen BFAO U.A.'], level = 'relatienaam')\n",
    "years = df.index.get_level_values(1).unique()\n",
    "names = df.index.get_level_values(0).unique()\n",
    "for name in names:\n",
    "    if (((name,years[0]) in df.index) & ((name,years[1]) in df.index) & ((name,years[2]) in df.index)) == False: \n",
    "        df = df.drop(name, level = 'relatienaam')\n",
    "X = df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we create a new DataFrame which consists of the changes over time and remove NaN too large numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame((X.loc[:,pd.IndexSlice[:,years[1]]]-X.loc[:,pd.IndexSlice[:,years[0]]].values)/X.loc[:,pd.IndexSlice[:,years[0]]].values).rename(columns = {years[1]:'change1617'})\n",
    "b = pd.DataFrame((X.loc[:,pd.IndexSlice[:,years[2]]]-X.loc[:,pd.IndexSlice[:,years[1]]].values)/X.loc[:,pd.IndexSlice[:,years[1]]].values).rename(columns = {years[2]:'change1718'})\n",
    "X2 = a.join(b).sort_index(axis = 1)\n",
    "X2.fillna(0, inplace = True)\n",
    "X2[X2 >= np.finfo(np.float64).max]= 1000\n",
    "X2[X2 == float('-inf')] = -1000\n",
    "X2[X2 == float('+inf')] = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we apply the t-sne algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = TSNE(n_components = 2, \n",
    "         perplexity = 1.0, \n",
    "         verbose = 0, \n",
    "         random_state = 0, \n",
    "         learning_rate = 20, \n",
    "         n_iter = 10000).fit_transform(X2.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results with 7 identified clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize = (7, 7))\n",
    "\n",
    "pyplot.scatter(x = Y[:, 0], \n",
    "              y = Y[:, 1], \n",
    "              s = 5)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 7, random_state = 0, n_init  = 10).fit(Y)\n",
    "\n",
    "for i in range(len(kmeans.cluster_centers_)):\n",
    "    \n",
    "    pyplot.scatter(x = kmeans.cluster_centers_[i,0],\n",
    "                   y = kmeans.cluster_centers_[i,1],\n",
    "                   s = 1,\n",
    "                   c = 'yellow')\n",
    "    \n",
    "    pyplot.annotate(str(i), \n",
    "                   xy = (kmeans.cluster_centers_[i, 0], kmeans.cluster_centers_[i, 1]), \n",
    "                   size = 13)\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, this does not yield very clear clusters. This is presumable because we do not have enough data since we only have two yearly differences available. Cluster 1 consists of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.T.loc[kmeans.labels_ == 5].index:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster contains index-linked and unit-linked assets and liabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.T.loc[kmeans.labels_ == 6].index:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cluster contains reinsurance recoverables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
